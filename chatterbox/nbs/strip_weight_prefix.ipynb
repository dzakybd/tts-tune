{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b725caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /home/germanwahnsinn/chatterbox/src/checkpoints/chatterbox_finetuned_hui/test2/t3_cfg.safetensors\n",
      "Original number of keys: 292\n",
      "  Keeping unstripped key (or key did not match prefix): cond_enc.emotion_adv_fc.weight\n",
      "  Keeping unstripped key (or key did not match prefix): cond_enc.perceiver.attn.norm.bias\n",
      "  Keeping unstripped key (or key did not match prefix): cond_enc.perceiver.attn.norm.weight\n",
      "  Keeping unstripped key (or key did not match prefix): cond_enc.perceiver.attn.proj_out.bias\n",
      "Warning: No keys found with the prefix 't3.'. The new state_dict will be empty or identical to the input if unstripped keys were kept.\n",
      "Saving all original keys as no prefix matched.\n",
      "Number of keys after stripping prefix 't3.': 292\n",
      "  0 keys had prefix stripped.\n",
      "  292 keys did not have the prefix (or were kept if uncommented).\n",
      "Successfully stripped prefix and saved new checkpoint to: /home/germanwahnsinn/chatterbox/src/checkpoints/test2/t3_cfg.safetensors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file, save_file\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "def strip_prefix_and_resave(checkpoint_path: str, prefix_to_strip: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Loads a PyTorch model checkpoint, strips a prefix from its state_dict keys,\n",
    "    and saves the modified state_dict to a new .safetensors file.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the input checkpoint file (.bin or .safetensors).\n",
    "        prefix_to_strip (str): The prefix to remove from the keys (e.g., \"t3.\").\n",
    "        output_path (str): Path to save the new .safetensors file.\n",
    "    \"\"\"\n",
    "    ckpt_path = Path(checkpoint_path)\n",
    "    out_path = Path(output_path)\n",
    "\n",
    "    if not ckpt_path.exists():\n",
    "        print(f\"Error: Checkpoint file not found at {ckpt_path}\")\n",
    "        return\n",
    "\n",
    "    if out_path.suffix != \".safetensors\":\n",
    "        print(f\"Warning: Output path '{out_path}' does not have a .safetensors extension. Saving as .safetensors anyway.\")\n",
    "        out_path = out_path.with_suffix(\".safetensors\")\n",
    "    \n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading checkpoint from: {ckpt_path}\")\n",
    "    if ckpt_path.suffix == \".safetensors\":\n",
    "        try:\n",
    "            state_dict = load_file(ckpt_path, device=\"cpu\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading .safetensors file: {e}\")\n",
    "            # Try loading as a full model state dict if it's a PEFT adapter or similar\n",
    "            try:\n",
    "                print(\"Attempting to load as a full model state dict which might contain adapter weights...\")\n",
    "                # This might be needed if the safetensors file is not just a flat state_dict\n",
    "                # but a more complex structure (less common for raw weights).\n",
    "                # For simple state_dict saves, load_file is usually enough.\n",
    "                # If it's an adapter, the keys might already be unprefixed.\n",
    "                data = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "                if \"state_dict\" in data:\n",
    "                    state_dict = data[\"state_dict\"]\n",
    "                elif isinstance(data, dict): # If it's already a state_dict\n",
    "                    state_dict = data\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported .safetensors structure for this script.\")\n",
    "            except Exception as e_alt:\n",
    "                print(f\"Alternative loading failed: {e_alt}\")\n",
    "                return\n",
    "    elif ckpt_path.suffix == \".bin\":\n",
    "        try:\n",
    "            data = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            # .bin files from Trainer might be the model itself or a dict containing state_dict\n",
    "            if isinstance(data, dict) and \"state_dict\" in data: # Common for Lightning checkpoints\n",
    "                state_dict = data[\"state_dict\"]\n",
    "            elif isinstance(data, dict) and not any(k.startswith(\"optimizer\") or k.startswith(\"lr_scheduler\") for k in data.keys()):\n",
    "                # Likely a raw state_dict (common for HF Trainer saves)\n",
    "                state_dict = data\n",
    "            elif hasattr(data, 'state_dict'): # If it's a model instance\n",
    "                 state_dict = data.state_dict()\n",
    "            else:\n",
    "                print(f\"Error: pytorch_model.bin does not seem to be a raw state_dict or contain a 'state_dict' key.\")\n",
    "                print(f\"         Content keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}\")\n",
    "                return\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading .bin file: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\"Error: Unsupported file extension '{ckpt_path.suffix}'. Please use .bin or .safetensors.\")\n",
    "        return\n",
    "\n",
    "    if not isinstance(state_dict, dict):\n",
    "        print(f\"Error: Loaded data is not a state_dict (dictionary). Type: {type(state_dict)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Original number of keys: {len(state_dict)}\")\n",
    "    # print(f\"Original keys sample: {list(state_dict.keys())[:5]}\")\n",
    "\n",
    "    new_state_dict = {}\n",
    "    stripped_count = 0\n",
    "    kept_count = 0\n",
    "\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith(prefix_to_strip):\n",
    "            new_key = key[len(prefix_to_strip):]\n",
    "            new_state_dict[new_key] = value\n",
    "            stripped_count += 1\n",
    "        else:\n",
    "            # If you only want the stripped part, you might not want to keep these.\n",
    "            # For creating a state_dict *only* for the sub-module, you'd typically ignore these.\n",
    "            # If the input checkpoint *only* contains keys for the sub-module (all prefixed),\n",
    "            # then this 'else' branch might not be hit much.\n",
    "            new_state_dict[key] = value\n",
    "            kept_count += 1 \n",
    "            if kept_count < 5 : # Log a few unstripped keys\n",
    "                 print(f\"  Keeping unstripped key (or key did not match prefix): {key}\")\n",
    "\n",
    "\n",
    "    if stripped_count == 0:\n",
    "        print(f\"Warning: No keys found with the prefix '{prefix_to_strip}'. The new state_dict will be empty or identical to the input if unstripped keys were kept.\")\n",
    "        if not new_state_dict and kept_count > 0: # only unstripped keys were kept by uncommenting\n",
    "             print(\"Saving all original keys as no prefix matched.\")\n",
    "             new_state_dict = state_dict # Fallback to save original if no stripping happened and unstripped were not kept\n",
    "        elif not new_state_dict and kept_count == 0:\n",
    "             print(\"Resulting state_dict is empty. Aborting save.\")\n",
    "             return\n",
    "\n",
    "\n",
    "    print(f\"Number of keys after stripping prefix '{prefix_to_strip}': {len(new_state_dict)}\")\n",
    "    print(f\"  {stripped_count} keys had prefix stripped.\")\n",
    "    print(f\"  {kept_count} keys did not have the prefix (or were kept if uncommented).\")\n",
    "    # print(f\"New keys sample: {list(new_state_dict.keys())[:5]}\")\n",
    "\n",
    "    if not new_state_dict and stripped_count > 0 : # Should not happen if stripped_count > 0\n",
    "        print(\"Error: Stripped keys but new_state_dict is empty. This is a bug.\")\n",
    "        return\n",
    "    if not new_state_dict and kept_count == 0 and stripped_count == 0:\n",
    "        print(\"No keys to save. Aborting.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    try:\n",
    "        save_file(new_state_dict, out_path)\n",
    "        print(f\"Successfully stripped prefix and saved new checkpoint to: {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving .safetensors file: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "strip_prefix_and_resave(\"path/to/checkpoint.safetensors\", \"t3.\", \"path/to/store/checkpoint.safetensors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatterbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
